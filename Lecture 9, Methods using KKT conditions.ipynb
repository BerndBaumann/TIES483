{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 9: Methods using KKT conditions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sequential Quadratic Programming (SQP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea is to generate a sequence of quadratic optimization problems whose solutions approach the solution of the original problem**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us consider problem\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t.Â }h_k(x) = 0\\text{ for all }k=1,\\ldots,K,\n",
    "$$\n",
    "where the the objective function and the equality constraints are twice differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we know that the optimal solution of this problem satisfies the KKT conditions, we know that\n",
    "$$\n",
    "\\left\\{\\begin{array}{l}\n",
    "\\nabla_xL(x,\\lambda,\\mu)=\\nabla f(x) + \\mu\\nabla h(x) = 0\\\\\n",
    "h(x) = 0\n",
    "\\end{array}\\right.\n",
    "$$\n",
    "Let us assume that we have a current esimation for the solution of the equality constraints $(x^k,\\mu^k)$, then according to the Newton's method for root finding (see e.g., https://en.wikipedia.org/wiki/Newton's_method), we have another solution $(x^k,\\mu^k)^T+(p,v)^T$ of the problem by solving system of equations\n",
    "$$\n",
    "HL(x,\\lambda,\\mu)\\left[\\begin{align}p^T\\\\v^T\\end{align}\\right] = -\\nabla L(x,\\lambda,\\mu).\n",
    "$$\n",
    "This can be written as\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "H_xL(x^k,\\lambda,\\mu^k)&\\nabla h(x^k)\\\\\n",
    "\\nabla h(x^k)^T & 0\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\left[\\begin{array}{c}p^T\\\\v^T\\end{array}\\right] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "-\\nabla_x L(x^k,\\lambda,\\mu^k)\\\\\n",
    "-h(x^k)^T\n",
    "\\end{array}\n",
    "\\right].\n",
    "$$\n",
    "\n",
    "However, the above is just the solution of the quadratic problem with equality constraints\n",
    "$$\n",
    "\\min \\frac12 p^THL(x^k,\\lambda,\\mu^k)p+\\nabla_xL(x^k,\\lambda,\\mu^k)\\\\\n",
    "\\text{s.t. }h_j(x^k) + \\nabla h_j(x^k)^Tp = 0. \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Intuitive interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are approximating the function quadratically around the current solution and the constraints are approximated linearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_constrained(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-len(x)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, [], [-1])\n"
     ]
    }
   ],
   "source": [
    "print f_constrained([1,0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def solve_SQP_equation(f,x):\n",
    "    \n",
    "\n",
    "\n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    f_old = float('inf')\n",
    "    f_new = f[0](x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        x = solve_SQP_equation(f,x)\n",
    "        f_new = f(x)\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lagrangian methods -- \"The original method of multipliers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us again consider problem\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }h_k(x) = 0\\text{ for all }k=1,\\ldots,K,\n",
    "$$\n",
    "where the the objective function and the equality constraints are twice differentiable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define augmented Lagrangian function\n",
    "$$\n",
    "L_c(x,\\mu) = f(x)+\\mu h(x)+\\frac12c\\|h(x)\\|^2.\n",
    "$$\n",
    "Above $c\\in \\mathbb R$ is a penalty parameter and $\\mu \\in \\mathbb R^n$ is a multiplier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let us consider sequence of optimization problems\n",
    "$$\n",
    "\\min_{x\\in\\mathbb R^n} f(x)+\\mu_k h(x)-\\frac{1}{2}c_k\\|h(x)\\|^2,\n",
    "$$\n",
    "where $c_{k+1}>c_k$ for $k=1,2,\\ldots$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if $\\mu_k=$ for all $k=1,2,\\ldots$, then we have a penalty function method, which solves the problem when $c_k\\to \\infty$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it can be shown, that if we set $\\mu_0$ randomly and keep on updating\n",
    "$\\mu_{k+1} = \\mu_k-c_kh(x)$, then we can show that there exists $C>0$ such that of $c_k>C$, then the optimal solution of the augmented Langrangian solves the original problem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us have optimization problem\n",
    "$$\n",
    "\\min x_1^2+x_2^2\\\\\n",
    "\\text{s.t. }x_1+x_2-1=0.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the minimizatio of the augmented Lagrangian becomes\n",
    "$$\n",
    "\\min_{x\\in\\mathbb R^n} x_1^2+x_2^2+\\mu_k(x_1+x_2-1)+\\frac12c_k(x_1+x_2-1)^2.\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f_constrained2(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augmented_langrangian(f,x,mu,c):\n",
    "    second_term = float(numpy.matrix(mu)*numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5*c*numpy.linalg.norm(f(x)[2])**2\n",
    "    return f(x)[0]-second_term+third_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def augmented_langrangian_method(f,start,mu0,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    mu = mu0\n",
    "    c = c0\n",
    "    while numpy.linalg.norm(f(x_new)[2])>0.00001:\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,mu,c),x_new)\n",
    "        x_old = x_new\n",
    "        mu = float(mu-numpy.matrix(c)*numpy.matrix(f(x_old)[2]).transpose())\n",
    "        x_new = res.x\n",
    "        c = 2*c\n",
    "    return x_new,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def penalty_function_method(f,start,c0):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    c = c0\n",
    "    while numpy.linalg.norm(f(x_new)[2])>0.00001:\n",
    "        res = minimize(lambda x:augmented_langrangian(f,x,0,c),x_new)\n",
    "        x_old = x_new\n",
    "        x_new = res.x\n",
    "        c = 2*c\n",
    "    return x_new,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.5,  0.5]), 2)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "augmented_langrangian_method(f_constrained2,[0,0],1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.49999618,  0.49999618]), 262144)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penalty_function_method(f_constrained2,[0,0],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## What is going on in here?\n",
    "\n",
    "This is not completely trivial, unfortunately. If you want to read details, please see e.g., http://www.mit.edu/~dimitrib/Constrained-Opt.pdf."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
