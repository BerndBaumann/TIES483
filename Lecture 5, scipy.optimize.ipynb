{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Optimization with scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we want to optimize something, we do not ofcourse need to start everything from scratch. It is good to know how algorithms work, but if the development of new algorithms is not the main point, then one can just use packages and libraries that have been premade.\n",
    "\n",
    "In Python, there are multiple packages for optimization. At this lecture, we are goint to take a look at *scipy.optimize* package.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Starting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "When we want to study a package in Python, we can import it.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to see the documentation, we can write the name of the package and two question marks and hit enter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "minimize??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization of multiple variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define again our friendly objective function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f_simple(x):\n",
    "    return (x[0] - 10.0)**2 + (x[1] + 5.0)**2+x[0]**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method: `Nelder-Mead'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation has the following to say:\n",
    "\n",
    "<pre>\n",
    "    Method :ref:`Nelder-Mead <optimize.minimize-neldermead>` uses the\n",
    "    Simplex algorithm [1]_, [2]_. This algorithm has been successful\n",
    "    in many applications but other algorithms using the first and/or\n",
    "    second derivatives information might be preferred for their better\n",
    "    performances and robustness in general.\n",
    "...\n",
    "     References\n",
    "    ----------\n",
    "    .. [1] Nelder, J A, and R Mead. 1965. A Simplex Method for Function\n",
    "        Minimization. The Computer Journal 7: 308-13.\n",
    "    .. [2] Wright M H. 1996. Direct search methods: Once scorned, now\n",
    "        respectable, in Numerical Analysis 1995: Proceedings of the 1995\n",
    "        Dundee Biennial Conference in Numerical Analysis (Eds. D F\n",
    "        Griffiths and G A Watson). Addison Wesley Longman, Harlow, UK.\n",
    "        191-208.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 50.000000\n",
      "         Iterations: 99\n",
      "         Function evaluations: 189\n",
      "[ 5.00003542 -4.99997315]\n"
     ]
    }
   ],
   "source": [
    "res = minimize(f_simple,[0,0],method='Nelder-Mead', \n",
    "         options={'disp': True})\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.optimize.optimize.Result'>\n",
      "  status: 0\n",
      "    nfev: 189\n",
      " success: True\n",
      "     fun: 50.000000003230383\n",
      "       x: array([ 5.00003542, -4.99997315])\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 99\n",
      "Optimization terminated successfully.\n"
     ]
    }
   ],
   "source": [
    "print type(res)\n",
    "print res\n",
    "print res.message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method: `CG`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation has the following to say:\n",
    "<pre>\n",
    "    Method :ref:`CG <optimize.minimize-cg>` uses a nonlinear conjugate\n",
    "    gradient algorithm by Polak and Ribiere, a variant of the\n",
    "    Fletcher-Reeves method described in [5]_ pp.  120-122. Only the\n",
    "    first derivatives are used.\n",
    "...\n",
    "   References\n",
    "    ----------\n",
    "...\n",
    "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
    "       Springer New York.\n",
    "</pre>\n",
    "The Conjugate gradient method needs the gradient. The documentation has the following to say\n",
    "<pre>\n",
    "    jac : bool or callable, optional\n",
    "        Jacobian (gradient) of objective function. Only for CG, BFGS,\n",
    "        Newton-CG, L-BFGS-B, TNC, SLSQP, dogleg, trust-ncg.\n",
    "        If `jac` is a Boolean and is True, `fun` is assumed to return the\n",
    "        gradient along with the objective function. If False, the\n",
    "        gradient will be estimated numerically.\n",
    "        `jac` can also be a callable returning the gradient of the\n",
    "        objective. In this case, it must accept the same arguments as `fun`.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Estimating the gradient numerically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 50.000000\n",
      "         Iterations: 2\n",
      "         Function evaluations: 20\n",
      "         Gradient evaluations: 5\n",
      "[ 5.00000003 -4.99999999]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "res = minimize(f_simple, [0,0], method='CG', #Conjugate gradient method\n",
    "               options={'disp': True})\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Giving the gradient with ad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 50.000000\n",
      "         Iterations: 2\n",
      "         Function evaluations: 5\n",
      "         Gradient evaluations: 5\n",
      "[ 5. -5.]\n"
     ]
    }
   ],
   "source": [
    "import ad\n",
    "res = minimize(f_simple, [0,0], method='CG', #Conjugate gradient method\n",
    "               options={'disp': True}, jac=ad.gh(f_simple)[0])\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method: `Newton-CG` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Newton-CG method uses a Newton-CG algorithm [5] pp. 168 (also known as the truncated Newton method). It uses a CG method to the compute the search direction. See also *TNC* method for a box-constrained minimization with a similar algorithm.\n",
    "\n",
    "   References\n",
    "    ----------\n",
    "    .. [5] Nocedal, J, and S J Wright. 2006. Numerical Optimization.\n",
    "       Springer New York.\n",
    "\n",
    "\n",
    "The Newton-CG algorithm needs the Jacobian and the Hessian. The documentation has the following to say:\n",
    "<pre>\n",
    "    hess, hessp : callable, optional\n",
    "        Hessian (matrix of second-order derivatives) of objective function or\n",
    "        Hessian of objective function times an arbitrary vector p.  Only for\n",
    "        Newton-CG, dogleg, trust-ncg.\n",
    "        Only one of `hessp` or `hess` needs to be given.  If `hess` is\n",
    "        provided, then `hessp` will be ignored.  If neither `hess` nor\n",
    "        `hessp` is provided, then the Hessian product will be approximated\n",
    "        using finite differences on `jac`. `hessp` must compute the Hessian\n",
    "        times an arbitrary vector.\n",
    " </pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Trying without reading the documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Jacobian is required for Newton-CG method",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-761f30559251>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1.3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m res = minimize(f_simple, [0,0], method='Newton-CG', #Newton-CG method\n\u001b[1;32m----> 4\u001b[1;33m                options={'disp': True})\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/scipy/optimize/_minimize.pyc\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001b[0m\n\u001b[0;32m    349\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'newton-cg'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m         return _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001b[1;32m--> 351\u001b[1;33m                                   **options)\n\u001b[0m\u001b[0;32m    352\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmeth\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'anneal'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_minimize_anneal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/scipy/optimize/optimize.pyc\u001b[0m in \u001b[0;36m_minimize_newtoncg\u001b[1;34m(fun, x0, args, jac, hess, hessp, callback, xtol, eps, maxiter, disp, return_all, **unknown_options)\u001b[0m\n\u001b[0;32m   1290\u001b[0m     \u001b[0m_check_unknown_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munknown_options\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mjac\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Jacobian is required for Newton-CG method'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[0mfprime\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjac\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Jacobian is required for Newton-CG method"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x0 = np.array([1.3, 0.7, 0.8, 1.9, 1.2])\n",
    "res = minimize(f_simple, [0,0], method='Newton-CG', #Newton-CG method\n",
    "               options={'disp': True})\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Giving the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 50.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 23\n",
      "         Hessian evaluations: 0\n",
      "[ 4.99999999 -4.99999999]\n"
     ]
    }
   ],
   "source": [
    "import ad\n",
    "res = minimize(f_simple, [0,0], method='Newton-CG', #Newton-CG method\n",
    "               options={'disp': True},jac=ad.gh(f_simple)[0])\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Giving also the hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 50.000000\n",
      "         Iterations: 7\n",
      "         Function evaluations: 8\n",
      "         Gradient evaluations: 7\n",
      "         Hessian evaluations: 7\n",
      "[ 5. -5.]\n"
     ]
    }
   ],
   "source": [
    "import ad\n",
    "res = minimize(f_simple, [0,0], method='Newton-CG', #Newton-CG method\n",
    "               options={'disp': True},jac=ad.gh(f_simple)[0],\n",
    "               hess=ad.gh(f_simple)[1])\n",
    "print res.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Line search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f_singlevar(x):\n",
    "    return 2+(1-x)**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar\n",
    "minimize_scalar??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method: `Golden`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation has the following to say:\n",
    "<pre>\n",
    "    Method :ref:`Golden <optimize.minimize_scalar-golden>` uses the\n",
    "    golden section search technique. It uses analog of the bisection\n",
    "    method to decrease the bracketed interval. It is usually\n",
    "    preferable to use the *Brent* method.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  fun: 2.0\n",
       "    x: 1.0\n",
       " nfev: 30"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_scalar(f_singlevar,method='golden',tol=0.00001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Method: `Brent`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The documentation has the following to say about the Brent method:\n",
    "\n",
    "    Method *Brent* uses Brent's algorithm to find a local minimum.\n",
    "    The algorithm uses inverse parabolic interpolation when possible to\n",
    "    speed up convergence of the golden section method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  fun: 2.0\n",
       " nfev: 5\n",
       "  nit: 4\n",
       "    x: 0.99999998519"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_scalar(f_singlevar,method='brent')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
