{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 7, direct methods for constrained optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Direct methods for constrained optimization are also known as *methods of feasible directions*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feasible descent directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $S\\subset \\mathbb R^n$ ($S\\neq \\emptyset$ closed) and $x^*\\in S$. \n",
    "**Definition:** The set\n",
    "$$ D = \\{d\\in \\mathbb R^n: d\\neq0,x^*+\\alpha d\\in S \\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the cone of feasible directions of $S$ in $x^*$.\n",
    "\n",
    "**Definition:** The set \n",
    "$$ F = \\{d\\in \\mathbb R^n: f(x^*+\\alpha d)<f(x^*)\\text{ for all } \\alpha\\in (0,\\delta) \\text{ for some } \\delta>0\\}$$\n",
    "is called the cone of descent directions.\n",
    "\n",
    "**Definition:** The set $F\\cap D$ is called the cone of feasible descent directions.\n",
    "\n",
    "![alt text](images/feasible_descent_directions.svg \"Feasible descent directions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Obvious) Theorem:** Consider an optimization problem \n",
    "$$\n",
    "\\begin{align}\n",
    "\\min &\\  f(x)\\\\\n",
    "\\text{s.t. }&\\ x\\in S\n",
    "\\end{align}\n",
    "$$\n",
    "and let $x^*\\in S$. Now if $x^*$ is a local minimizer **then** the set of feasible descent directions $F\\cap D$ is empty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea for the methods of feasible descent directions\n",
    "\n",
    "1. Assume a feasible solution $x$.\n",
    "2. Find a feasible descent direction $d\\in D\\cap F$.\n",
    "3. Determine the step length to the direction $d$\n",
    "4. Update $x$ accordingly.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Projected gradient method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume a problem with linear equality constraints\n",
    "$$\n",
    "\\min f(x)\\\\\n",
    "\\text{s.t. }Ax=b,\n",
    "$$\n",
    "where $A$ is a $lxn$ matrix ($l\\leq n$) and $b$ is a vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let $x$ be a feasible solution to the above problem.\n",
    "\n",
    "It holds that:\n",
    "\n",
    "$d$ is a feasible direction *if and only if* $Ad=0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, the gradient $\\nabla f(x)$ is a feasible descent direction, if \n",
    "$$ A\\nabla f(x)=0.$$\n",
    "\n",
    "This may or may not be true.\n",
    "\n",
    "However, we can project the gradient to the set of feasible descent directions\n",
    "$$ \\{d\\in \\mathbb R^n: Ad=0\\},$$\n",
    "which now is a linear subspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Projection\n",
    "\n",
    "Let $a\\in \\mathbb R^n$ be a vector and let $L$ be a linear subspace of $\\mathbb R^n$. Now, the following are equivalent\n",
    "* $a^P$ is the projection of $a$ on $L$,\n",
    "* $\\{a^P\\} = \\operatorname{argmin}_{l\\in L}\\|a-l\\|$, and\n",
    "* $a^P\\in A$ and $(a-a^P)^Tl=0$ for all $l\\in L$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Projected gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The projection of the gradient $\\nabla f(x)$ on the set $\\{d\\in \\mathbb R^n: Ad=0\\}$ is denoted by $\\nabla f(x)^P$ and called the *projected gradient*. \n",
    "\n",
    "Now, given some conditions, the projected gradient gives us a feasible descent direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## How to compute the projected gradient?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways, but at this course we can use optimization. Basically, the optimization problem that we have to solve is\n",
    "$$\n",
    "\\min \\|\\nabla f(x)-d\\|\\\\\n",
    "\\text{s.t. }Ad=0.\n",
    "$$\n",
    "\n",
    "Since it is equivalent to minimize the square of the objective function, we can see that the problem is a quadratic problem with inequality constraints, which means that we just need to solve the system of equations\n",
    "$$\n",
    "\\left[\n",
    "\\begin{array}{cc}\n",
    "0.5*I&A^T\\\\\n",
    "A&0\n",
    "\\end{array}\n",
    "\\right] d= \\left[ \n",
    "\\begin{array}{c}\n",
    "\\nabla f(x)\\\\\n",
    "0\n",
    "\\end{array}\n",
    "\\right],\n",
    "$$\n",
    "where I is the identity matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Code in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function for projecting a vector to a linear space defined by $Ax=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def project_vector(A,vector):\n",
    "    #convert A into a matrix\n",
    "    A_matrix = np.matrix(A)\n",
    "    #construct the \"first row\" of the matrix [[I,A^T],[A,0]]\n",
    "    left_matrix_first_row = np.concatenate((0.5*np.identity(len(vector)),A_matrix.transpose()), axis=1)\n",
    "    #construct the \"second row\" of the matrix\n",
    "    left_matrix_second_row = np.concatenate((A_matrix,np.matrix(np.zeros([len(A),len(vector)+len(A)-len(A[0])]))), axis=1)\n",
    "    #combine the whole matrix by combining the rows\n",
    "    left_matrix = np.concatenate((left_matrix_first_row,left_matrix_second_row),axis = 0)\n",
    "    #Solve the system of linear equalities from the previous page\n",
    "    return np.linalg.solve(left_matrix, np.concatenate((np.matrix(vector).transpose(),np.zeros([len(A),1])),axis=0))[:len(vector)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.],\n",
       "        [-2.]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = [[1,0]]\n",
    "gradient = [-1,-1]\n",
    "project_vector(A,gradient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us study optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad& x_1^2+x_2^2+x_3^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1+x_2=3\\\\\n",
    "    &x_1+x_3=4.\n",
    "\\end{align}\n",
    "$$\n",
    "Let us project a gradient from a feasible point $x=(1,2,3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the matrix\n",
    "$$\n",
    "A = \\left[\n",
    "\\begin{array}{ccc}\n",
    "1& 1 & 0\\\\\n",
    "1& 0 & 1\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.33333333]\n",
      " [-5.33333333]\n",
      " [-5.33333333]]\n"
     ]
    }
   ],
   "source": [
    "import ad\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "gradient = ad.gh(lambda x:x[0]**2+x[1]**2+x[2]**2)[0]([1,2,3])\n",
    "d = project_vector(A,[-i for i in gradient])\n",
    "print d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d is a feasible direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.],\n",
       "        [ 0.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.matrix(A)*d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x[0]**2+x[1]**2+x[2]**2\n",
    "alpha = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d is a descent direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of f at [1,2,3] is 14\n",
      "Value of f at [1,2,4] +alpha*d is 13.9574186667\n"
     ]
    }
   ],
   "source": [
    "print \"Value of f at [1,2,3] is \"+str(f([1,2,3]))\n",
    "x_mod= np.array([1,2,3])+alpha*np.array(d).transpose()[0]\n",
    "print \"Value of f at [1,2,4] +alpha*d is \"+str(f(x_mod))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Finally, the algorithm of the projected gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "def projected_gradient_method(f,A,start,step,precision):\n",
    "    f_old = float('Inf')\n",
    "    x = np.array(start)\n",
    "    steps = []\n",
    "    f_new = f(x)\n",
    "    while abs(f_old-f_new)>precision:\n",
    "        f_old = f_new\n",
    "        gradient = ad.gh(f)[0](x)\n",
    "        d = project_vector(A,[-i for i in gradient])#The only changes to steepest..\n",
    "        d = np.array(d.transpose())[0] #... descent are here!\n",
    "        x = x+d*step\n",
    "        f_new = f(x)\n",
    "        steps.append(list(x))\n",
    "    return x,f_new,steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ad/__init__.py:767: RuntimeWarning: overflow encountered in double_scalars\n",
      "  qc_wrt_args = [y*(y - 1)*x**(y - 2), x**y*(math.log(x))**2]\n",
      "/usr/lib/python2.7/site-packages/ad/__init__.py:87: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  qc_wrt_vars[var1] += dh*f.d2(var1) + d2h*fdv1**2\n",
      "/usr/lib/python2.7/site-packages/ad/__init__.py:768: RuntimeWarning: overflow encountered in double_scalars\n",
      "  cp_wrt_args = x**y*(y*math.log(x) + 1)/x\n",
      "/usr/lib/python2.7/site-packages/ad/__init__.py:97: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  tmp = 2*cp_wrt_args*ad_funcs[0].d(var1)*ad_funcs[1].d(var1)\n",
      "/usr/lib/python2.7/site-packages/ad/__init__.py:766: RuntimeWarning: overflow encountered in double_scalars\n",
      "  lc_wrt_args = [y*x**(y - 1), x**y*math.log(x)]\n",
      "/usr/lib/python2.7/site-packages/ad/__init__.py:84: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  lc_wrt_vars[var1] += dh*fdv1\n"
     ]
    }
   ],
   "source": [
    "f = lambda x:x[0]**2+x[1]**2+x[2]**2\n",
    "A = [[1,1,0],[1,0,1]]\n",
    "start = [1,2,3]\n",
    "(x,f_val,steps) = projected_gradient_method(f,A,start,0.6,0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.333248  0.666752  1.666752]\n",
      "[[ 3.]\n",
      " [ 4.]]\n"
     ]
    }
   ],
   "source": [
    "print x\n",
    "print np.matrix(A)*np.matrix(x).transpose()"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
